{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7vIHyuIYw6A",
        "outputId": "da98c73a-8f49-45b4-aa42-7adaba5e54e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/drive/MyDrive/NLP/train.csv\n",
        "# /content/drive/MyDrive/NLP/valid.csv"
      ],
      "metadata": {
        "id": "cy8p681jY1to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import regex\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "a5LETrHOY1wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageUrduTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.merges = {}\n",
        "\n",
        "        self.regex_pattern = r\"\"\" ?\\p{Arabic}+| ?\\p{N}+| ?[^\\s\\p{Arabic}\\p{N}]+|\\s+\"\"\" #regex\n",
        "\n",
        "        self.compiled_regex = regex.compile(self.regex_pattern)\n",
        "\n",
        "    def _get_stats(self, ids):\n",
        "        counts = {}\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def _merge_vocab(self, pair, idx, ids):\n",
        "        new_ids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "                new_ids.append(idx); i += 2\n",
        "            else:\n",
        "                new_ids.append(ids[i]); i += 1\n",
        "        return new_ids\n",
        "\n",
        "    def train(self, text, vocab_size=5000, transition_pct=0.9):\n",
        "        print(f\"--- Training (Vocab: {vocab_size} | Split: {transition_pct}) ---\")\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        self.merges = {}\n",
        "\n",
        "        # Subword\n",
        "        text_chunks = regex.findall(self.compiled_regex, text)\n",
        "        ids_list = [list(chunk.encode(\"utf-8\")) for chunk in text_chunks]\n",
        "\n",
        "        num_merges = vocab_size - 256\n",
        "        stage_1_limit = int(num_merges * transition_pct)\n",
        "        stage_2_limit = num_merges - stage_1_limit\n",
        "\n",
        "        print(f\"Stage 1: Learning {stage_1_limit} subword merges...\")\n",
        "        for i in range(stage_1_limit):\n",
        "            stats = {}\n",
        "            for chunk_ids in ids_list:\n",
        "                chunk_stats = self._get_stats(chunk_ids)\n",
        "                for pair, freq in chunk_stats.items():\n",
        "                    stats[pair] = stats.get(pair, 0) + freq\n",
        "            if not stats: break\n",
        "            pair = max(stats, key=stats.get)\n",
        "            idx = 256 + len(self.merges)\n",
        "            ids_list = [self._merge_vocab(pair, idx, chunk) for chunk in ids_list]\n",
        "            self.merges[pair] = idx\n",
        "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
        "\n",
        "        # Superword\n",
        "        print(f\"Stage 2: Learning {stage_2_limit} superword merges...\")\n",
        "        flat_ids = [token for chunk in ids_list for token in chunk]\n",
        "        for i in range(stage_2_limit):\n",
        "            stats = self._get_stats(flat_ids)\n",
        "            if not stats: break\n",
        "            pair = max(stats, key=stats.get)\n",
        "            idx = 256 + len(self.merges)\n",
        "            flat_ids = self._merge_vocab(pair, idx, flat_ids)\n",
        "            self.merges[pair] = idx\n",
        "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
        "\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        ids = list(text.encode(\"utf-8\"))\n",
        "        while True:\n",
        "            stats = self._get_stats(ids)\n",
        "            if not stats: break\n",
        "            valid_pairs = {p: self.merges[p] for p in stats if p in self.merges}\n",
        "            if not valid_pairs: break\n",
        "            best_pair = min(valid_pairs, key=valid_pairs.get)\n",
        "            idx = self.merges[best_pair]\n",
        "            ids = self._merge_vocab(best_pair, idx, ids)\n",
        "        return ids\n"
      ],
      "metadata": {
        "id": "vtutlRoKZSOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA CLEANING\n",
        "def get_clean_corpus(csv_path):\n",
        "    print(f\"Loading and normalizing {csv_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        col = 'headline' if 'headline' in df.columns else df.columns[0]\n",
        "        raw_data = df[col].astype(str).tolist()\n",
        "\n",
        "        urdu_pattern = regex.compile(r'[\\u0600-\\u06ff]')\n",
        "        cleaned = []\n",
        "        for line in raw_data:\n",
        "            # nfkc normalization\n",
        "            norm = unicodedata.normalize('NFKC', line).strip()\n",
        "            if urdu_pattern.search(norm):\n",
        "                cleaned.append(norm)\n",
        "        return \"\\n\".join(cleaned)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "rH6wdfj3ZSRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execution\n",
        "train_text = get_clean_corpus('/content/drive/MyDrive/NLP/train.csv')\n",
        "valid_text = get_clean_corpus('/content/drive/MyDrive/NLP/valid.csv')\n",
        "\n",
        "# if not train_text:\n",
        "#     train_text = \"سچن تیندولکر کا وراٹ کوہلی کو مشورہ، یہ غلطی کبھی مت کرنا۔ \" * 500\n",
        "#     valid_text = \"سچن تیندولکر کا وراٹ کوہلی\"\n",
        "\n",
        "print(\"\\n>>> TRAINING BASELINE MODEL <<<\")\n",
        "baseline_tok = TwoStageUrduTokenizer()\n",
        "baseline_tok.train(train_text, vocab_size=2000, transition_pct=1.0)\n",
        "\n",
        "print(\"\\n>>> TRAINING IST MODEL <<<\")\n",
        "ist_tok = TwoStageUrduTokenizer()\n",
        "ist_tok.train(train_text, vocab_size=2000, transition_pct=0.8)\n",
        "\n",
        "# fertility score\n",
        "def get_fertility(model, text):\n",
        "    words = text.split()\n",
        "    if not words: return 0\n",
        "    tokens = model.encode(text)\n",
        "    return len(tokens) / len(words)\n",
        "\n",
        "print(\"\\n================ RESULTS ================\")\n",
        "base_score = get_fertility(baseline_tok, valid_text)\n",
        "ist_score = get_fertility(ist_tok, valid_text)\n",
        "\n",
        "print(f\"Baseline Fertility: {base_score:.4f}\")\n",
        "print(f\"IST Fertility:      {ist_score:.4f}\")\n",
        "\n",
        "if base_score > 0:\n",
        "    imp = ((base_score - ist_score) / base_score) * 100\n",
        "    print(f\"Improvement:        {imp:.2f}%\")\n",
        "    print(\"Interpretation: IST represents the text using fewer tokens.\")\n",
        "print(\"=========================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDwW3RRtZSUW",
        "outputId": "ee0317a5-b15a-4fc7-f754-9e3e7108b562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and normalizing /content/drive/MyDrive/NLP/train.csv...\n",
            "Loading and normalizing /content/drive/MyDrive/NLP/valid.csv...\n",
            "\n",
            ">>> TRAINING BASELINE MODEL <<<\n",
            "--- Training (Vocab: 2000 | Split: 1.0) ---\n",
            "Stage 1: Learning 1744 subword merges...\n",
            "Stage 2: Learning 0 superword merges...\n",
            "Training Complete.\n",
            "\n",
            ">>> TRAINING IST MODEL <<<\n",
            "--- Training (Vocab: 2000 | Split: 0.8) ---\n",
            "Stage 1: Learning 1395 subword merges...\n",
            "Stage 2: Learning 349 superword merges...\n",
            "Training Complete.\n",
            "\n",
            "================ RESULTS ================\n",
            "Baseline Fertility: 1.5082\n",
            "IST Fertility:      1.4014\n",
            "Improvement:        7.08%\n",
            "Interpretation: IST represents the text using fewer tokens.\n",
            "=========================================\n"
          ]
        }
      ]
    }
  ]
}